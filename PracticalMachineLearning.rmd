---
title: "PracticalMachineLearning"
output: html_document
---

Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

Let's first read in the training and test files. 
```{r,echo=FALSE}

dir <- "C:/Users/lwakjira/Documents/fy16 training/Data Science/PracticalMachineLearning/"
specdata <- setwd(dir)
options('download.file.method'='curl')

library(data.table)
training<-fread(input="./data/pml-training.csv",sep=",",na.strings=c("NA","","#DIV/0!"))
testing<-fread(input="./data/pml-testing.csv",sep=",",na.strings=c("NA","","#DIV/0!"))

```
#Data Processing
Convert columns that are character to numeric. e.g. kurtosis_yaw_dumbbell
Next, let's check if there are any fields with zero values using near zero var function.
Also, remove columns with NA values for more than 95% of the data. Use is.na(colSum) function. 
Let's load needed packages necessary to complete this project. 
```{r,echo=FALSE}
library(caret)
NZV_training<-nearZeroVar(training,saveMetrics=TRUE)
NZV_training[NZV_training$zeroVar=="TRUE",]
training_nz <-subset(training, select=-c(amplitude_yaw_forearm, skewness_yaw_forearm,kurtosis_yaw_forearm,amplitude_yaw_dumbbell,skewness_yaw_dumbbell,kurtosis_yaw_dumbbell,kurtosis_yaw_belt,skewness_yaw_belt ))
training_nz1 <- subset(training_nz,select=-c(1:7))
training_nz1$kurtosis_roll_belt <-as.numeric(as.character(training_nz1$kurtosis_roll_belt),as.is=TRUE)
training_nz1$min_yaw_dumbbell<-as.numeric(as.character(training_nz1$min_yaw_dumbbell))
training_nz1$max_yaw_dumbbell<-as.numeric(as.character(training_nz1$max_yaw_dumbbell))
training_nz1$skewness_pitch_dumbbell<-as.numeric(as.character(training_nz1$skewness_pitch_dumbbell))
training_nz1$skewness_roll_dumbbell<-as.numeric(as.character(training_nz1$skewness_roll_dumbbell))
training_nz1$kurtosis_picth_dumbbell<-as.numeric(as.character(training_nz1$kurtosis_picth_dumbbell))
training_nz1$kurtosis_roll_dumbbell<-as.numeric(as.character(training_nz1$kurtosis_roll_dumbbell))
training_nz1$kurtosis_roll_arm      <-as.numeric(as.character(training_nz1$kurtosis_roll_arm))
training_nz1$kurtosis_picth_arm <-as.numeric(as.character(training_nz1$kurtosis_picth_arm))
training_nz1$kurtosis_yaw_arm <-as.numeric(as.character(training_nz1$kurtosis_yaw_arm))
training_nz1$skewness_roll_arm  <-as.numeric(as.character(training_nz1$skewness_roll_arm))
training_nz1$skewness_pitch_arm <-as.numeric(as.character(training_nz1$skewness_pitch_arm))
training_nz1$skewness_yaw_arm <-as.numeric(as.character(training_nz1$skewness_yaw_arm))
training_nz1$amplitude_yaw_belt<-as.numeric(as.character(training_nz1$amplitude_yaw_belt))
training_nz1$min_yaw_belt <-as.numeric(as.character(training_nz1$min_yaw_belt))
training_nz1$max_yaw_belt<-as.numeric(as.character(training_nz1$max_yaw_belt))
training_nz1$kurtosis_picth_belt<-as.numeric(as.character(training_nz1$kurtosis_picth_belt))
training_nz1$skewness_roll_belt  <-as.numeric(as.character(training_nz1$skewness_roll_belt))
training_nz1$skewness_roll_belt.1<-as.numeric(as.character(training_nz1$skewness_roll_belt.1))
training_nz1$kurtosis_roll_forearm<-as.numeric(as.character(training_nz1$kurtosis_roll_forearm))
training_nz1$kurtosis_picth_forearm<-as.numeric(as.character(training_nz1$kurtosis_picth_forearm))
training_nz1$skewness_roll_forearm  <-as.numeric(as.character(training_nz1$skewness_roll_forearm))
training_nz1$skewness_pitch_forearm <-as.numeric(as.character(training_nz1$skewness_pitch_forearm))
training_nz1$skewness_roll_forearm  <-as.numeric(as.character(training_nz1$skewness_roll_forearm))
training_nz1$skewness_pitch_forearm <-as.numeric(as.character(training_nz1$skewness_pitch_forearm))
training_nz1$min_yaw_forearm <-as.numeric(as.character(training_nz1$min_yaw_forearm))
training_nz1$max_yaw_forearm<-as.numeric(as.character(training_nz1$max_yaw_forearm))

training_nz1<-data.frame(training_nz1)
todrop<-which(colSums(is.na(training_nz1))>=19216)
training_nz2<-training_nz1[,-c(todrop)]
print("Variables to be used in the model")
str(training_nz2)
```
There are 8 variables with all zero values. So, I have decided to drop those variables.
There were 27 variables that came in as character, which I converted to numeric. 
Also, I am now left with 53 variables after removing variables with >=19216 NA values, which is more than 95% of the data.

#Model Building & Validation
Let's divide the training data to two so we can test our model on 25% of the training data before applying it to test data. 
Then, build a model using train function with rainforest method and cross validation. Also, use seed to be able to reproduce the same result. 
```{r,echo=FALSE}
inTrain<-createDataPartition(y=training_nz2$classe,p=0.75,list=FALSE)
training1 <- training_nz2[inTrain[,1],]
training2<-training_nz2[-inTrain[,1],]
print("Below are dimensions for training subset 1 & training subset 2")
dim(training1);dim(training2)

set.seed(96356)
train_control <- trainControl(method="cv", number=3)
model <- train(classe ~ .,data=training1,method='rf',trControl=train_control,prox=TRUE,allowParallel=TRUE)
print("Below is the Model output - Note accuracy above 98% for all mtry.")
print(model,digits=3)
#apply model to training set 2
cls_predictTrain2 <- predict(model, training2)                                            
#check how the prediction is compared to actual
print("See below for actual vs prediction when model is applied to training subset 2")
confusionMatrix(cls_predictTrain2, training2$classe)
```
#Test set Prediction
The best accuracy is 99.3%, which means that the out of sample error is <1%.
Let's apply the model to the 20 test data. 
```{r,echo=FALSE}
ans1<-predict(model, testing)  
ans1
#ans1 preparation for submission
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(ans1)

```
#Conclusion
After cleaning the data for fields with mostly NA values, converting variables to numeric, and creating prediction model using the rain forest method and 3 k-folds, we have accuracy rate of 99.33%. The out of sample error rate was <1%, which is a very good model. It is necessary though to do a more thorough investigation of the 53 variables in the model to see if there is correlation. 
Great exercise, which took a lot of my time, but worth it! I hope you had a great time too. 
